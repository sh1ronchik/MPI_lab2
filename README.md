# Отчет по лабораторной работе №2

# Состав команды
Работу выполнили студенты группы **22ПИ1**:

| Участник | Вклад в проект |
|---------|----------------|
| **Овсянников Артём Сергеевич** | Реализовал задание **1**: вычисление множества Мандельброта с использованием OpenMP, а также задание **2.1**: решение задачи N тел с использованием OpenMP. |
| **Шейх Руслан Халедович** |  Отладил  задание **2.1**: решение задачи N тел с использованием OpenMP. Также Реализовал задания **2.2**: решение задачи N тел с использованием CUDA и **3**: создание своей реализации типа rwlock и функций rdlock, wrlock блокировки чтения-записи |

# Руководство по настройке OpenMP проекта

## 1. Установка (Ubuntu)
```bash
sudo apt update
sudo apt install -y build-essential gcc libomp-dev
```

### Проверка поддержки OpenMP
```bash
gcc --version
echo | gcc -fopenmp -x c -E -dM - | grep -i openmp
```



## Компиляция всех заданий

```bash
chmod +x scripts/compile_all.sh
./scripts/compile_all.sh
```

## Запуск бенчмарков

```bash
# Task 1
chmod +x task1/scripts/run_benchmarks.sh
./task1/scripts/run_benchmarks.sh

# Task 2 OpenMP
chmod +x task2/scripts/run_benchmarks.sh
./task2/scripts/run_benchmarks.sh

# Task 2 CUDA
chmod +x task2/scripts/run_benchmarks_cuda.sh
./task2/scripts/run_benchmarks_cuda.sh

# Task 3
chmod +x task3/scripts/run_comparison.sh
./task3/scripts/run_comparison.sh
```

## Задание 1: Множество Мандельброта (OpenMP)

### Описание задачи

Необходимо написать параллельную программу с использованием OpenMP для нахождения множества Мандельброта.

**Множество Мандельброта** — это совокупность всех комплексных чисел $$c$$, для которых последовательность:

$$z_{n+1} = z_n^2 + c, \quad z_1 = 0$$

является ограниченной для всех $$n \in \mathbb{N}$$.

Число $$c$$ принадлежит множеству Мандельброта тогда и только тогда, когда:

$$(∀n ∈ \mathbb{N}) \quad |z_n| < 2$$

### Реализация

Программа реализована в файле `task1/scripts/task1.c` с использованием следующих подходов:

#### Основные компоненты:

1. **Параллелизация вычислений:**
   ```c
   #pragma omp parallel for schedule(dynamic, 100)
   for (long long i = 0; i < grid_dim; i++) {
       // Проверка каждой точки
   }
   ```

2. **Локальные буферы для потоков:**
   - Каждый поток работает со своим локальным буфером
   - Минимизация конфликтов при доступе к памяти
   - Избежание false sharing

3. **Критические секции:**
   ```c
   #pragma omp critical
   {
       // Объединение локальных результатов в глобальный массив
   }
   ```

4. **Динамическое планирование:**
   - `schedule(dynamic, 100)` — для балансировки нагрузки
   - Размер чанка 100 итераций подобран эмпирически

#### Параметры вычислений:

- Область комплексной плоскости: $$[-2.5, 1.0] \times [-1.0, 1.0]$$
- Максимальное число итераций: 1000
- Радиус отсечения: 2.0
- Размер сетки: $$\sqrt{npoints} \times \sqrt{npoints}$$

### Запуск и тестирование

#### Компиляция:
```bash
gcc -fopenmp -O3 -o task1/scripts/task1 task1/scripts/task1.c -lm
```

#### Примеры запуска:
```bash
# С 4 потоками и 10 миллионами точек
./task1/scripts/task1 4 10000000

# С усреднением по 5 запускам
./task1/scripts/task1 8 10000000 5
```

#### Автоматический бенчмарк:
```bash
chmod +x task1/scripts/run_benchmarks.sh
./task1/scripts/run_benchmarks.sh
```

### Результаты замеров производительности

**Платформа:** AMD EPYC 7B12  
**Сетка:** 3162 × 3162 (фактически 9 998 244 точки, запрошено 10 000 000)  
**Найдено точек внутри множества:** 2 157 278 → 21.58% (не зависит от числа потоков)  
**Методика измерений:** 3 запуска на каждом наборе параметров. Для расчёта ускорения и эффективности использовано среднее время (`avg_time`). 

#### Результаты (средние времена, 3 прогона)

| Потоки | Время (среднее) | Мин (с) | Макс (с) | Ускорение (T₁ / Tₙ) | Эффективность |
|--------|----------------|---------|----------|--------------------|---------------|
| 1      | 8.0596         | 8.0194  | 8.1081   | 1.00×              | 1.00          |
| 2      | 4.9367         | 4.6719  | 5.2123   | 1.63×              | 0.82          |
| 4      | 5.0351         | 4.7295  | 5.3851   | 1.60×              | 0.40          |
| 8      | 4.9439         | 4.7477  | 5.3215   | 1.63×              | 0.20          |
| 16     | 5.0146         | 4.7894  | 5.3249   | 1.61×              | 0.10          |


#### Выводы

- Максимальное ускорение наблюдается при 2 потоках, далее увеличение числа потоков почти не даёт прироста.  
- Эффективность параллельного выполнения резко падает с ростом числа потоков, что указывает вероятно на высокие накладные расходы синхронизации.


### Выходные данные

- **`task1/data/result.csv`** — координаты точек множества (real, imaginary)
- **`task1/data/task1_performance.csv`** — метрики производительности


---

## Задание 2: Задача N тел (OpenMP и CUDA)

### Описание задачи

Дано N материальных точек с массами $$m_k$$, начальными положениями $$\vec{r}_k$$ и скоростями $$\vec{v}_k$$. Требуется определить траектории всех частиц под действием взаимного гравитационного притяжения.

### Теоретическая основа

#### Закон всемирного тяготения:

Сила, действующая на тело q со стороны тела k:

$$\vec{F}_{qk} = G \frac{m_q m_k}{|\vec{r}_k - \vec{r}_q|^3} (\vec{r}_k - \vec{r}_q)$$

где $$G = 6.67430 \times 10^{-11}$$ м³/(кг·с²) — гравитационная постоянная.

Общая сила на тело q:

$$\vec{F}_q = \sum_{k=1, k \neq q}^{N} \vec{F}_{qk}$$

#### Метод Эйлера 1-го порядка:

Система дифференциальных уравнений:

$$\frac{d\vec{r}_q}{dt} = \vec{v}_q$$

$$m_q \frac{d\vec{v}_q}{dt} = \vec{F}_q$$

Дискретизация (с шагом $$\Delta t$$):

$$\vec{r}_q^{n} = \vec{r}_q^{n-1} + \vec{v}_q^{n-1} \Delta t$$

$$\vec{v}_q^{n} = \vec{v}_q^{n-1} + \frac{\vec{F}_q^{n-1}}{m_q} \Delta t$$

### Реализация OpenMP

Файл: `task2/scripts/task2.c`

#### Ключевые особенности:

1. **Параллельное вычисление сил с использованием третьего закона Ньютона:**
   ```c
   #pragma omp for schedule(static)
        for (int i = 0; i < n - 1; i++) {
            ...
            for (int j = i + 1; j < n; j++) {
                ...
   ```
   - Каждая пара тел обрабатывается один раз

2. **Параллельное обновление позиций:**
   ```c
   #pragma omp parallel for schedule(static)
   for (int i = 0; i < n; i++) {
       // Обновление скорости и позиции
   }
   ```

#### Параметры симуляции:

- Шаг по времени: $$\Delta t = 0.01 с$$
- Интервал вывода: каждый 10-й шаг
- Формат вывода: CSV с траекториями всех тел

#### Компиляция и запуск:

```bash
# Компиляция
gcc -fopenmp -O3 -o task2/scripts/task2 task2/scripts/task2.c -lm

# Запуск
./task2/scripts/task2 4 100.0 task2/data/input/three_body.txt
```

### Результаты замеров производительности

**Платформа:** AMD EPYC 7B12  
**Задача:** Моделирование движения 1000 тел методом Эйлера 1-го порядка  
**Методика измерений:** 3 запуска на каждом наборе параметров. Для расчёта ускорения и эффективности использовано среднее время (`avg_time`).  

#### Результаты (средние времена, 3 прогона)

| Потоки | Время (среднее) | Мин (с) | Макс (с) | Ускорение (T₁ / Tₙ) | Эффективность |
|--------|----------------|---------|----------|--------------------|---------------|
| 1      | 3.8501         | 3.6718  | 4.1755   | 1.00×              | 1.00          |
| 2      | 3.6504         | 3.4500  | 3.9503   | 1.05×              | 0.53          |
| 4      | 3.4001         | 3.2105  | 3.7209   | 1.13×              | 0.28          |
| 8      | 3.2509         | 3.0503  | 3.5506   | 1.18×              | 0.15          |
| 16     | 3.1005         | 2.9103  | 3.4509   | 1.24×              | 0.08          |

#### Выводы

- Небольшое ускорение наблюдается при росте числа потоков, при этом максимальный эффект достигается при 16 потоках (~1.24×).  
- Эффективность параллельного выполнения быстро падает с увеличением числа потоков, что указывает на высокие накладные расходы синхронизации и ограниченную нагрузку на потоки.  


### Реализация CUDA

Файл: `task2/scripts/task2_cuda.cu`

#### Архитектура GPU решения:

1. **Kernel для вычисления сил:**
   ```cuda
   __global__ void compute_forces_kernel(Body *bodies, float *fx, float *fy, float *fz, int n) {
       int idx = blockIdx.x * blockDim.x + threadIdx.x;
       // Каждый поток вычисляет силу для одного тела
   }
   ```

2. **Kernel для обновления состояния:**
   ```cuda
   __global__ void update_bodies_kernel(Body *bodies, float *fx, float *fy, float *fz, 
                                        int n, float dt) {
       int idx = blockIdx.x * blockDim.x + threadIdx.x;
       // Каждый поток обновляет одно тело
   }
   ```

#### CUDA оптимизации:

1. **Использование float вместо double:**
   - Ускорение вычислений на GPU
   - Компромисс между точностью и производительностью

2. **Softening параметр** ($$\epsilon = 10^{-9}$$):
   - Избежание сингулярностей: $$r^2 \rightarrow r^2 + \epsilon$$
   - Стабилизация численного решения

3. **Минимизация копирований Host↔Device:**
   - Данные остаются на GPU в течение всей симуляции
   - Копирование только для записи промежуточных результатов

4. **Параметры запуска:**
   - Block size: 256 потоков
   - Grid size: вычисляется динамически `(N + 255) / 256`

#### Компиляция и запуск:

```bash
# Компиляция
nvcc -O3 -o task2/scripts/task2_cuda task2/scripts/task2_cuda.cu -lm

# Запуск
./task2/scripts/task2_cuda 100.0 task2/data/input/three_body.txt
```

### Результаты производительности

Тесты на симуляции 1000 тел в течение 10 секунд (1000 шагов, dt = 0.01):

| Реализация         | Время (сек) | Ускорение относительно 1 потока OpenMP |
|-------------------|-------------|----------------------------------------|
| OpenMP (1 поток)  | 3.8501      | 1.00×  |
| OpenMP (2 потока) | 3.6504      | 1.05×  |
| OpenMP (4 потока) | 3.4001      | 1.13×  |
| OpenMP (8 потоков)| 3.2509      | 1.18×  |
| OpenMP (16 потоков)| 3.1005     | 1.24×  |
| **CUDA (Tesla T4)** | 0.0527    | **≈73×** |

*Примечание:* CUDA версия использовала блоки по 256 потоков и сетку из 2 блоков. Для больших N (>1000) преимущество CUDA становится ещё более значительным.

#### Анализ

1. **CUDA показывает драматическое ускорение** благодаря:
   - Массивному параллелизму (тысячи потоков на GPU)
   - Оптимизированным математическим операциям на GPU
   - Эффективной работе с глобальной и shared памятью

2. **OpenMP версия:**
   - Показала умеренное ускорение при росте числа потоков
   - Максимальная эффективность достигается при 1–2 потоках
   - Легче разрабатывать и отлаживать на CPU

3. **Выбор технологии зависит от:**
   - Размера задачи (N)
   - Доступного оборудования
   - Требований к времени выполнения и точности


### Выходные данные

- **OpenMP:** `task2/data/result.csv`
- **CUDA:** `task2/data/result_cuda.csv`
- **Метрики:** `task2/data/task2_openmp_performance.csv` и `task2_cuda_performance.csv`



---

## Задание 3: Read-Write блокировки (Pthreads)

### Описание задачи

Создать собственную реализацию типа `rwlock` и функций блокировки чтения-записи. Сравнить производительность с библиотечной реализацией `pthread_rwlock_t`.

### Теоретическая основа

#### Концепция Read-Write блокировок:

**Блокировка чтения (read lock):**
- Множественные потоки могут одновременно получить блокировку на чтение
- Блокируется только при наличии активного писателя

**Блокировка записи (write lock):**
- Эксклюзивная — только один поток может владеть
- Блокирует всех читателей и других писателей

#### Проблема голодания:

Если постоянно приходят читатели, писатели могут никогда не получить доступ. Решение — **приоритет писателям**.

### Архитектура реализации

Файлы:
- `task3/scripts/my_rwlock.h` — интерфейс
- `task3/scripts/my_rwlock.c` — реализация
- `task3/scripts/task3_my_rwlock.c` — тестирование

#### Структура данных:

```c
typedef struct {
    pthread_mutex_t mutex;              /* Защита структуры */
    pthread_cond_t read_cond;           /* CV для читателей */
    pthread_cond_t write_cond;          /* CV для писателей */
    int active_readers;                 /* Количество активных читателей */
    int waiting_readers;                /* Ожидающие читатели */
    int waiting_writers;                /* Ожидающие писатели */
    int writer_active;                  /* Флаг активного писателя */
} my_rwlock_t;
```

### Алгоритм работы

#### Блокировка на чтение (`my_rwlock_rdlock`):

1. Захватить мьютекс
2. Увеличить счетчик ожидающих читателей
3. **Ждать**, пока:
   - Нет активного писателя
   - Нет ожидающих писателей (приоритет!)
4. Уменьшить счетчик ожидающих
5. Увеличить счетчик активных читателей
6. Освободить мьютекс

```c
int my_rwlock_rdlock(my_rwlock_t *rwlock) {
    pthread_mutex_lock(&rwlock->mutex);
    
    rwlock->waiting_readers++;
    while (rwlock->writer_active || rwlock->waiting_writers > 0) {
        pthread_cond_wait(&rwlock->read_cond, &rwlock->mutex);
    }
    rwlock->waiting_readers--;
    rwlock->active_readers++;
    
    pthread_mutex_unlock(&rwlock->mutex);
    return 0;
}
```

#### Блокировка на запись (`my_rwlock_wrlock`):

1. Захватить мьютекс
2. Увеличить счетчик ожидающих писателей
3. **Ждать**, пока:
   - Нет активных читателей
   - Нет активного писателя
4. Уменьшить счетчик ожидающих
5. Установить флаг активного писателя
6. Освободить мьютекс

```c
int my_rwlock_wrlock(my_rwlock_t *rwlock) {
    pthread_mutex_lock(&rwlock->mutex);
    
    rwlock->waiting_writers++;
    while (rwlock->active_readers > 0 || rwlock->writer_active) {
        pthread_cond_wait(&rwlock->write_cond, &rwlock->mutex);
    }
    rwlock->waiting_writers--;
    rwlock->writer_active = 1;
    
    pthread_mutex_unlock(&rwlock->mutex);
    return 0;
}
```

#### Освобождение (`my_rwlock_unlock`):

1. Захватить мьютекс
2. Определить тип блокировки (чтение/запись)
3. Обновить счетчики
4. **Политика пробуждения:**
   - Если был писатель и есть ожидающие писатели → разбудить одного писателя
   - Если был писатель и нет ожидающих писателей → разбудить всех читателей
   - Если был последний читатель → разбудить писателя
5. Освободить мьютекс

```c
int my_rwlock_unlock(my_rwlock_t *rwlock) {
    pthread_mutex_lock(&rwlock->mutex);
    
    if (rwlock->writer_active) {
        rwlock->writer_active = 0;
        if (rwlock->waiting_writers > 0) {
            pthread_cond_signal(&rwlock->write_cond);
        } else {
            pthread_cond_broadcast(&rwlock->read_cond);
        }
    } else if (rwlock->active_readers > 0) {
        rwlock->active_readers--;
        if (rwlock->active_readers == 0 && rwlock->waiting_writers > 0) {
            pthread_cond_signal(&rwlock->write_cond);
        }
    }
    
    pthread_mutex_unlock(&rwlock->mutex);
    return 0;
}
```

### Тестовая программа

Используется **односвязный список** с операциями:
- **Member** (поиск) — блокировка на чтение
- **Insert** (вставка) — блокировка на запись
- **Delete** (удаление) — блокировка на запись

#### Компиляция:

```bash
# Пользовательская реализация
gcc -pthread -O3 -o task3/scripts/task3_my_rwlock \
    task3/scripts/task3_my_rwlock.c task3/scripts/my_rwlock.c -lm

# Библиотечная реализация
gcc -pthread -O3 -o task3/scripts/task3_pthread_rwlock \
    task3/scripts/task3_pthread_rwlock.c -lm
```

#### Запуск сравнения:

```bash
chmod +x task3/scripts/run_comparison.sh
./task3/scripts/run_comparison.sh
```

Скрипт выполняет два теста:
1. **Много читателей:** 90% поиск, 5% вставка, 5% удаление
2. **Сбалансированная нагрузка:** 50% поиск, 25% вставка, 25% удаление

### Результаты сравнения

#### Тест 1: Преобладание чтения (90% read, 10% write)

Конфигурация: 1000 начальных ключей, 100000 операций

| Потоки | My RWLock (сек) | Pthread RWLock (сек) | Разница |
|--------|-----------------|----------------------|---------|
| 1      | 0.523           | 0.519                | +0.8%   |
| 2      | 0.466           | 0.499                | -6.6%   |
| 4      | 0.541           | 0.542                | -0.2%   |
| 8      | 0.643           | 0.569                | +13.0%  |

#### Тест 2: Сбалансированная нагрузка (50% read, 50% write)

| Потоки | My RWLock (сек) | Pthread RWLock (сек) | Разница |
|--------|-----------------|----------------------|---------|
| 1      | 2.474           | 2.456                | +0.7%   |
| 2      | 2.928           | 3.584                | -18.3%  |
| 4      | 3.182           | 3.440                | -7.5%   |
| 8      | 3.781           | 3.833                | -1.4%   |

### Анализ результатов

#### Наблюдения

- Близкая производительность при малом количестве потоков (1-2)
- Разрыв между реализациями меняется в зависимости от нагрузки и числа потоков
- Наибольшая разница наблюдается при 8 потоках в тесте с преобладанием чтения (My RWLock чуть медленнее)

#### Причины различий

**Преимущества `pthread_rwlock_t`:**
- Оптимизированные системные вызовы
- Возможное использование аппаратных инструкций (atomic operations)
- Адаптивные алгоритмы в зависимости от платформы
- Встроенные оптимизации компилятора

**Накладные расходы `my_rwlock_t`:**
- Дополнительные проверки условий
- Больше операций с мьютексом
- Потенциально больше пробуждений потоков

#### Выводы

- Собственная реализация корректна — все операции выполняются правильно
- Производительность приемлема — разница небольшая, кроме отдельных случаев многопоточной нагрузки

---
